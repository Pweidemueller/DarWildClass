{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First test to see if pretrained models are already good enough to classify our DarWild animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the resolution of the clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## doesn't work in VSCode interactive\n",
    "# def display_frame(frame):\n",
    "#     cv2.imshow(\"Frame\", frame)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "def extract_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def crop_frames(frame, pixel_frombottom=100):\n",
    "    return frame[:-pixel_frombottom, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path):\n",
    "    frames = extract_frames(video_path)\n",
    "    print(f'The clip contains {len(frames)} frames.')\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '../images/DSCF0935.MP4'\n",
    "\n",
    "# Extract frames from the video\n",
    "frames = extract_frames(video_path)\n",
    "print(f'The clip contains {len(frames)} frames.')\n",
    "\n",
    "# for i, frame in enumerate(frames):\n",
    "#     # Print resolution\n",
    "#     resolution = (frame.shape[1], frame.shape[0])  # (width, height)\n",
    "#     print(f\"Frame {i + 1} resolution: {resolution}\")\n",
    "\n",
    "#     # Print RGB channels\n",
    "#     channels = frame.shape[2]  # Number of channels (3 for RGB)\n",
    "#     print(f\"Frame {i + 1} channels: {channels}\")\n",
    "\n",
    "#     # Convert frame to numpy array with desired shape\n",
    "#     frame_array = np.array(frame.transpose(2, 0, 1))  # (channels, height, width)\n",
    "#     print(f\"Frame {i + 1} array shape: {frame_array.shape}\")\n",
    "\n",
    "#     # Save frame_array to file if needed\n",
    "#     # np.save(f\"frame_{i + 1}.npy\", frame_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained model\n",
    "Pretrained models can be found here: https://github.com/huggingface/pytorch-image-models\n",
    "They've all been trained on the same collection of images (IMAGENET) with 1000 labels.\n",
    "\n",
    "Available models and their performance can be found here: https://github.com/huggingface/pytorch-image-models/blob/main/results/results-imagenet.csv\n",
    "\n",
    "According to Artem we want:\n",
    "1. high top1/5 score\n",
    "2. small param_count (too many parameters would make the model very large and not easy to run on my laptop)\n",
    "\n",
    "Quickstart guide: https://huggingface.co/docs/timm/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_pil(numpy_image):\n",
    "    # opencv loads frames as BGR rather than RGB but PIL expects RGB\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(numpy_image, cv2.COLOR_BGR2RGB))\n",
    "    return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WildClip:\n",
    "    def __init__(self, filepath, animal_exp):\n",
    "        self.filepath = filepath\n",
    "        self.expectedanimal = animal_exp\n",
    "\n",
    "    def get_frames(self):\n",
    "        self.frames = extract_frames(self.filepath)\n",
    "        print(f'The clip contains {len(self.frames)} frames.')\n",
    "\n",
    "    def classify_image(self, frame, transform, model):\n",
    "        # remove the time stamp banner\n",
    "        frame_crop = crop_frames(frame, pixel_frombottom=110)\n",
    "        pil_image = numpy_to_pil(frame_crop)\n",
    "        image_tensor = transform(pil_image)\n",
    "\n",
    "        # We use unsqueeze(0) in this case, as the model is expecting a batch dimension.\n",
    "        # don't keep track of the gradients to not run out of memory\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor.unsqueeze(0))\n",
    "\n",
    "        # To get the predicted probabilities, we apply softmax to the output. This leaves us with a tensor of shape (num_classes,).\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        # get the top5 predictions\n",
    "        values, indices = torch.topk(probabilities, 5)\n",
    "\n",
    "        return(values, indices)\n",
    "\n",
    "    def find_animals_clip(self, transform, model, IMAGENET_1k_LABELS):\n",
    "        predictions = []\n",
    "        for i, frame in enumerate(self.frames):\n",
    "            if i % 20 == 0:\n",
    "                print(i)\n",
    "            prob, val = self.classify_image(frame, transform, model)\n",
    "            predictions += [(prob,val)]\n",
    "        majority_vote = Counter([j for i in predictions for j in i[1].numpy()])\n",
    "        indeces = sorted([(i, j) for i, j in majority_vote.items()], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # If we check the imagenet labels for the top index, we can see what the model predictedâ€¦\n",
    "        print(f'For {self.expectedanimal} model finds following classes:')\n",
    "        print([{'label': IMAGENET_1k_LABELS[idx[0]], 'number of frames:': idx[1]} for idx in indeces])\n",
    "\n",
    "        self.predictions=predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get imagenet labels\n",
    "IMAGENET_1k_URL = 'https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt'\n",
    "IMAGENET_1k_LABELS = requests.get(IMAGENET_1k_URL).text.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('tf_efficientnet_b5.ns_jft_in1k', pretrained=True)\n",
    "# Note: The returned PyTorch model is set to train mode by default, so you must call .eval() on it if you plan to use it for inference.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out what  transforms where applied for model inputs\n",
    "# important parameters: resolution and normalisation\n",
    "# model.pretrained_cfg\n",
    "timm.data.resolve_data_config(model.pretrained_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transform that transforms images into the right input format\n",
    "data_cfg = timm.data.resolve_data_config(model.pretrained_cfg)\n",
    "transform = timm.data.create_transform(**data_cfg)\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ducks = WildClip('../images/DSCF0935.MP4', animal_exp='ducks')\n",
    "Ducks.get_frames()\n",
    "numpy_to_pil(Ducks.frames[4]).save('../images/Ducks.png')\n",
    "# Ducks.find_animals_clip(transform, model, IMAGENET_1k_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Otter = WildClip('../images/DSCF0005.MP4', animal_exp='otter')\n",
    "Otter.get_frames()\n",
    "numpy_to_pil(Otter.frames[1]).save('../images/Otter.png')\n",
    "\n",
    "# Otter.find_animals_clip(transform, model, IMAGENET_1k_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Squirrel = WildClip('../images/DSCF0006.MP4', animal_exp='squirrel')\n",
    "Squirrel.get_frames()\n",
    "numpy_to_pil(Squirrel.frames[50]).save('../images/Squirrel.png')\n",
    "\n",
    "# Squirrel.find_animals_clip(transform, model, IMAGENET_1k_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mouse = WildClip('../images/DSCF0115.MP4', animal_exp='mouse')\n",
    "Mouse.get_frames()\n",
    "numpy_to_pil(Mouse.frames[100]).save('../images/Mouse.png')\n",
    "\n",
    "# Mouse.find_animals_clip(transform, model, IMAGENET_1k_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pigeon = WildClip('../images/DSCF0025.MP4', animal_exp='pigeon')\n",
    "Pigeon.get_frames()\n",
    "numpy_to_pil(Pigeon.frames[200]).save('../images/Pigeon.png')\n",
    "\n",
    "# Pigeon.find_animals_clip(transform, model, IMAGENET_1k_LABELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
